# Линейные модели в машинном обучении

## 1. Основные понятия

### 1.1 Выборка (Dataset)

**Выборка** — это набор данных, который мы используем для обучения и тестирования модели машинного обучения.

Выборка состоит из:
- **Обучающей выборки** — данные для тренировки модели
- **Тестовой выборки** — данные для проверки качества модели
- **Валидационной выборки** — данные для настройки гиперпараметров

**Пример выборки для предсказания цены недвижимости:**

| Площадь (м²) | Количество комнат | Этаж | Район | Цена (млн руб.) |
|--------------|-------------------|------|-------|-----------------|
| 45           | 2                 | 3    | Центр | 8.5             |
| 60           | 3                 | 1    | Спальный | 5.2         |
| 35           | 1                 | 5    | Центр | 6.8             |
| 80           | 4                 | 2    | Элитный | 12.3          |

В этом примере у нас есть 4 объекта (квартиры), каждый описывается 4 признаками, и для каждого известна целевая переменная (цена).

### 1.2 Фичи (Features/Признаки)

**Фичи** — это характеристики объектов, которые мы используем для предсказания. Это входные переменные модели.

**Типы фичей:**
1. **Числовые (непрерывные)** — площадь, возраст, температура
2. **Категориальные** — район, тип дома, марка автомобиля
3. **Бинарные** — есть балкон (да/нет), новостройка (да/нет)
4. **Порядковые** — оценка (плохо/удовлетворительно/хорошо/отлично)

**Обозначения:**
- Матрица признаков: **X** размером n×m (n объектов, m признаков)
- Вектор признаков одного объекта: **x** = (x₁, x₂, ..., xₘ)

**Пример преобразования категориальных признаков:**

Исходный признак "Район":
- Центр → [1, 0, 0]
- Спальный → [0, 1, 0]  
- Элитный → [0, 0, 1]

Это называется **one-hot encoding**.

### 1.3 Таргет (Target/Целевая переменная)

**Таргет** — это то, что мы хотим предсказать. Это выходная переменная модели.

**Обозначения:**
- Вектор целевых значений: **y** = (y₁, y₂, ..., yₙ)
- Предсказанные значения: **ŷ** = (ŷ₁, ŷ₂, ..., ŷₙ)

**Типы таргетов:**
1. **Непрерывные** (регрессия) — цена, температура, доходность
2. **Дискретные** (классификация) — класс объекта, категория

## 2. Модели машинного обучения

### 2.1 Что такое модель?

**Модель** — это математическая функция, которая преобразует входные данные (признаки) в выходные (предсказания).

Общий вид: **f: X → Y**

где:
- X — пространство признаков
- Y — пространство ответов
- f — функция модели

### 2.2 Линейная модель

**Линейная модель** предполагает, что между признаками и таргетом существует линейная зависимость.

**Математическая формулировка:**

ŷ = w₀ + w₁x₁ + w₂x₂ + ... + wₘxₘ

или в векторном виде:

ŷ = **w**ᵀ**x** + w₀

где:
- **w** = (w₁, w₂, ..., wₘ) — вектор весов
- w₀ — смещение (bias)
- **x** — вектор признаков

**Наглядный пример для одного признака:**

Предсказание цены квартиры по площади:
```
Цена = w₀ + w₁ × Площадь
Цена = 2.5 + 0.1 × Площадь

Для квартиры 50 м²:
Цена = 2.5 + 0.1 × 50 = 7.5 млн руб.
```

На графике это прямая линия:
```
Цена |
(млн)|     /
 10  |    /
     |   /
  5  |  /
     | /
  0  |/________
     0  50  100 Площадь (м²)
```

### 2.3 Веса модели

**Веса (параметры)** — это числа, которые определяют важность каждого признака.

**Интерпретация весов:**
- **Положительный вес** — увеличение признака увеличивает таргет
- **Отрицательный вес** — увеличение признака уменьшает таргет
- **Большой по модулю вес** — признак сильно влияет на результат
- **Маленький вес** — признак слабо влияет на результат

**Пример:**
```
Цена = 3.0 + 0.08×Площадь + 0.5×Комнаты - 0.2×Этаж + 2.0×Центр

Интерпретация:
- Увеличение площади на 1 м² → цена +80 тыс. руб.
- Дополнительная комната → цена +500 тыс. руб.
- Каждый этаж выше → цена -200 тыс. руб.
- Центральный район → цена +2 млн руб.
```

### 2.4 Пространство признаков

**Пространство признаков** — это многомерное пространство, где каждая ось соответствует одному признаку.

**Для 2D случая:**
```
Признак 2 |
          |   • объект 3
    4     |     
          | • объект 1
    2     |
          |       • объект 2
    0     |________________
          0    2    4    6  Признак 1
```

**Линейная модель в пространстве признаков:**
- **1D** — прямая линия
- **2D** — прямая на плоскости  
- **3D** — плоскость в пространстве
- **nD** — гиперплоскость в n-мерном пространстве

## 3. Задачи машинного обучения

### 3.1 Задача регрессии

**Регрессия** — предсказание непрерывных значений.

**Примеры задач регрессии:**
- Предсказание цены недвижимости
- Прогноз температуры
- Оценка доходности акций
- Предсказание возраста по фото

**Линейная регрессия:**

ŷ = w₀ + w₁x₁ + w₂x₂ + ... + wₘxₘ

**Графический пример (1D):**
```
y |           • (фактическое значение)
  |         /
  |       /  ŷ (предсказание)
  |     /    |
  |   /      |
  | /        •
  |/         |
  |__________|_______ x
             |
          остаток (ошибка)
```

### 3.2 Задача классификации

**Классификация** — предсказание дискретных классов.

**Примеры задач классификации:**
- Определение спама в email
- Диагностика заболеваний
- Распознавание изображений
- Анализ тональности текста

**Виды классификации:**
1. **Бинарная** — 2 класса (спам/не спам)
2. **Многоклассовая** — много классов (кот/собака/птица)
3. **Многометочная** — несколько меток одновременно

**Линейная классификация:**

Для бинарной классификации используем **сигмоиду**:

P(y=1|x) = σ(w₀ + w₁x₁ + ... + wₘxₘ)

где σ(z) = 1/(1 + e⁻ᶻ) — сигмоидная функция

**График сигмоиды:**
```
P(y=1) |
   1   |     ___----
       |   _/
  0.5  | _/
       |/
   0   |____________ z
      -∞      0     ∞
```

## 4. Меры близости и функции потерь

### 4.1 Что такое функция потерь?

**Функция потерь (Loss function)** — функция, которая измеряет, насколько сильно предсказания модели отличаются от истинных значений.

Обозначение: L(y, ŷ)

где:
- y — истинное значение
- ŷ — предсказанное значение

**Цель обучения:** минимизировать среднюю функцию потерь по всей выборке.

### 4.2 Функции потерь для регрессии

#### 4.2.1 Среднеквадратичная ошибка (MSE)

L(y, ŷ) = (y - ŷ)²

**Средняя по выборке:**
MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²

**Свойства MSE:**
- Сильно штрафует большие ошибки
- Дифференцируема везде
- Чувствительна к выбросам

**Пример:**
```
Истинные значения: [3, 5, 2, 8]
Предсказания:      [3.1, 4.8, 2.3, 7.5]
Ошибки:           [0.1, 0.2, 0.3, 0.5]
Квадраты ошибок:  [0.01, 0.04, 0.09, 0.25]
MSE = (0.01 + 0.04 + 0.09 + 0.25) / 4 = 0.0975
```

#### 4.2.2 Средняя абсолютная ошибка (MAE)

L(y, ŷ) = |y - ŷ|

**Средняя по выборке:**
MAE = (1/n) × Σᵢ₌₁ⁿ |yᵢ - ŷᵢ|

**Свойства MAE:**
- Равномерно штрафует все ошибки
- Менее чувствительна к выбросам
- Не дифференцируема в нуле

#### 4.2.3 Функция Хьюбера

Комбинирует MSE и MAE:

L(y, ŷ) = {
  (1/2)(y - ŷ)²,     если |y - ŷ| ≤ δ
  δ|y - ŷ| - (1/2)δ², если |y - ŷ| > δ
}

**График сравнения функций потерь:**
```
Loss |
     |     MSE (парабола)
  4  |    /
     |   /    
  2  |  /    Huber
     | /    /
  0  |/__MAE (прямая)
     |   |    |
    -2   0    2  Error
```

### 4.3 Функции потерь для классификации

#### 4.3.1 Логистическая функция потерь

Для бинарной классификации:

L(y, ŷ) = -[y × log(ŷ) + (1-y) × log(1-ŷ)]

где:
- y ∈ {0, 1} — истинный класс
- ŷ ∈ [0, 1] — предсказанная вероятность класса 1

**Интуиция:**
- Если y=1 и ŷ→1, то loss→0
- Если y=1 и ŷ→0, то loss→∞
- Если y=0 и ŷ→0, то loss→0  
- Если y=0 и ŷ→1, то loss→∞

#### 4.3.2 Hinge Loss (для SVM)

L(y, ŷ) = max(0, 1 - y × ŷ)

где:
- y ∈ {-1, +1} — истинный класс
- ŷ — предсказанное значение (до применения функции активации)

**График Hinge Loss:**
```
Loss |
  2  |\
     | \
  1  |  \
     |   \____
  0  |        \______
     |              
    -2  -1   0   1   2  y×ŷ
```

## 5. Математические основы оптимизации

### 5.1 Пределы

**Предел функции** — значение, к которому стремится функция при приближении аргумента к определенной точке.

**Обозначение:** lim(x→a) f(x) = L

**Примеры:**
```
1) lim(x→2) (x² + 1) = 4 + 1 = 5

2) lim(x→0) (sin(x)/x) = 1

3) lim(x→∞) (1/x) = 0
```

**Зачем нужны в ML:**
- Понимание поведения функции потерь
- Анализ сходимости алгоритмов оптимизации
- Обоснование методов градиентного спуска

### 5.2 Производные

**Производная** показывает скорость изменения функции в данной точке.

**Определение:**
f'(x) = lim(h→0) [f(x+h) - f(x)] / h

**Геометрический смысл:** тангенс угла наклона касательной.

**Основные правила дифференцирования:**
```
1) (c)' = 0 (константа)
2) (x^n)' = n×x^(n-1)
3) (af(x))' = a×f'(x)
4) (f(x) + g(x))' = f'(x) + g'(x)
5) (f(x)×g(x))' = f'(x)×g(x) + f(x)×g'(x)
6) (f(g(x)))' = f'(g(x))×g'(x) (цепное правило)
```

**Примеры вычисления производных:**
```
1) f(x) = x² → f'(x) = 2x
2) f(x) = 3x³ + 2x - 5 → f'(x) = 9x² + 2
3) f(x) = e^x → f'(x) = e^x
4) f(x) = ln(x) → f'(x) = 1/x
5) f(x) = sin(x) → f'(x) = cos(x)
```

**Применение в ML:**
```
Функция потерь MSE: L(w) = (y - wx)²
Производная: dL/dw = -2x(y - wx)

При y=5, x=2, w=1:
L(1) = (5 - 1×2)² = 9
dL/dw = -2×2×(5 - 1×2) = -4×3 = -12

Отрицательная производная → нужно увеличить w
```

### 5.3 Дифференциалы

**Дифференциал** — главная линейная часть приращения функции.

df = f'(x)dx

**Геометрический смысл:** приращение функции вдоль касательной.

**Применение:**
```
Если f(x) = x² и x = 3, dx = 0.1:
df = f'(3)×0.1 = 6×0.1 = 0.6

Точное значение: f(3.1) - f(3) = 9.61 - 9 = 0.61
Приближение через дифференциал: 0.6
```

### 5.4 Градиент

**Градиент** — вектор, составленный из частных производных функции многих переменных.

Для функции f(x₁, x₂, ..., xₙ):

∇f = (∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ)

**Геометрический смысл:** направление наибыстрейшего возрастания функции.

**Пример для функции двух переменных:**
```
f(x, y) = x² + 2y² + xy

∂f/∂x = 2x + y
∂f/∂y = 4y + x

∇f = (2x + y, 4y + x)

В точке (1, 2):
∇f(1, 2) = (2×1 + 2, 4×2 + 1) = (4, 9)
```

**Свойства градиента:**
- Направлен в сторону наибыстрейшего возрастания
- Перпендикулярен линиям уровня функции
- В точках экстремума равен нулю

**Визуализация градиента:**
```
f(x,y) |
       |    ↗ ∇f (градиент)
   z   |   /
       |  •  точка
       | /
       |/_________
         x        y
```

## 6. Оптимизация функций потерь

### 6.1 Постановка задачи оптимизации

**Цель:** найти такие веса w, которые минимизируют функцию потерь.

min L(w) = min (1/n) × Σᵢ₌₁ⁿ l(yᵢ, f(xᵢ, w))

где:
- L(w) — функция потерь, зависящая от весов
- l — функция потерь для одного примера
- f(xᵢ, w) — предсказание модели для примера i

### 6.2 Аналитическое решение

**Условие минимума:** ∇L(w) = 0

Для функций, где это возможно решить аналитически.

### 6.3 Градиентный спуск

**Идея:** двигаемся в направлении, противоположном градиенту.

**Алгоритм:**
```
1. Инициализируем веса w⁽⁰⁾
2. На каждой итерации t:
   - Вычисляем градиент: g⁽ᵗ⁾ = ∇L(w⁽ᵗ⁾)
   - Обновляем веса: w⁽ᵗ⁺¹⁾ = w⁽ᵗ⁾ - η × g⁽ᵗ⁾
3. Повторяем до сходимости
```

где η (эта) — скорость обучения (learning rate).

**Визуализация градиентного спуска:**
```
Loss |     
     |    •w₀ (начальная точка)
     |   / \
     |  /   \
     | /     \  
     |/   •w₁ \
     |      \  \
     |     •w₂ \
     |          \
     |         •w₃ (минимум)
     |___________________ w
```

**Выбор скорости обучения:**
- **Слишком большая η** → алгоритм расходится
- **Слишком маленькая η** → медленная сходимость
- **Оптимальная η** → быстрая и стабильная сходимость

### 6.4 Стохастический градиентный спуск (SGD)

**Проблема обычного GD:** для больших данных вычисление градиента по всей выборке дорого.

**Решение SGD:** на каждой итерации используем только один случайный пример.

**Алгоритм SGD:**
```
1. Инициализируем веса w⁽⁰⁾
2. На каждой итерации t:
   - Выбираем случайный пример (xᵢ, yᵢ)
   - Вычисляем градиент: g⁽ᵗ⁾ = ∇l(yᵢ, f(xᵢ, w⁽ᵗ⁾))
   - Обновляем веса: w⁽ᵗ⁺¹⁾ = w⁽ᵗ⁾ - η × g⁽ᵗ⁾
```

**Mini-batch SGD:** компромисс между GD и SGD, используем небольшие батчи.

## 7. Метод наименьших квадратов (МНК)

### 7.1 Постановка задачи

Для линейной регрессии y = Xw + ε хотим найти веса w, минимизирующие сумму квадратов остатков.

**Функция потерь:**
L(w) = ||y - Xw||² = (y - Xw)ᵀ(y - Xw)

### 7.2 Вывод аналитического решения

Раскрываем квадрат:
L(w) = yᵀy - 2wᵀXᵀy + wᵀXᵀXw

Берем производную по w:
∇L(w) = -2Xᵀy + 2XᵀXw

Приравниваем к нулю:
-2Xᵀy + 2XᵀXw = 0
XᵀXw = Xᵀy

**Аналитическое решение:**
w* = (XᵀX)⁻¹Xᵀy

### 7.3 Геометрическая интерпретация

МНК находит ортогональную проекцию вектора y на пространство, натянутое на столбцы матрицы X.

**Визуализация (2D случай):**
```
y |     • (истинное значение)
  |    /|
  |   / |
  |  /  | остаток
  | /   |
  |/    • (проекция)
  |______|_______ X
         
  Проекция = предсказание модели
```

### 7.4 Условия применимости МНК

1. **XᵀX обратима** — нет линейной зависимости между признаками
2. **n > m** — объектов больше, чем признаков
3. **Линейная зависимость** — истинная зависимость близка к линейной
4. **Гомоскедастичность** — дисперсия шума постоянна
5. **Независимость остатков** — ошибки некоррелированы

### 7.5 Пример вычисления МНК

**Данные:**
```
X = [1, 1]    y = [3]
    [1, 2]        [5]
    [1, 3]        [7]
```

**Вычисления:**
```
XᵀX = [1, 1, 1] × [1, 1] = [3, 6]
      [1, 2, 3]   [1, 2]   [6, 14]
                  [1, 3]

Xᵀy = [1, 1, 1] × [3] = [15]
      [1, 2, 3]   [5]   [31]
                  [7]

(XᵀX)⁻¹ = 1/(3×14-6×6) × [14, -6] = 1/6 × [14, -6] = [7/3, -1]
                          [-6,  3]        [-6,  3]   [-1, 1/2]

w* = [7/3, -1] × [15] = [7/3×15 - 31] = [35 - 31] = [1]
     [-1, 1/2]   [31]   [-15 + 31/2]   [-15 + 15.5]  [2]
```

**Результат:** w₀ = 1, w₁ = 2

**Модель:** ŷ = 1 + 2x

**Проверка:**
- При x=1: ŷ = 1 + 2×1 = 3 ✓
- При x=2: ŷ = 1 + 2×2 = 5 ✓  
- При x=3: ŷ = 1 + 2×3 = 7 ✓

## 8. Наглядные примеры

### 8.1 Пример 1: Предсказание зарплаты

**Задача:** Предсказать зарплату программиста по опыту работы.

**Данные:**
| Опыт (лет) | Зарплата (тыс. руб.) |
|------------|----------------------|
| 1          | 60                   |
| 2          | 80                   |
| 3          | 100                  |
| 4          | 120                  |
| 5          | 140                  |

**Модель:** Зарплата = w₀ + w₁ × Опыт

**Применяем МНК:**
```
X = [1, 1]    y = [60]
    [1, 2]        [80]
    [1, 3]        [100]
    [1, 4]        [120]
    [1, 5]        [140]

После вычислений: w₀ = 40, w₁ = 20
```

**Итоговая модель:** Зарплата = 40 + 20 × Опыт

**Интерпретация:**
- Базовая зарплата: 40 тыс. руб.
- За каждый год опыта: +20 тыс. руб.

### 8.2 Пример 2: Классификация email

**Задача:** Определить, является ли письмо спамом.

**Признаки:**
- Количество восклицательных знаков
- Длина темы письма
- Наличие слова "бесплатно"

**Данные:**
| Восклиц. знаки | Длина темы | "Бесплатно" | Спам? |
|----------------|------------|-------------|-------|
| 0              | 10         | 0           | 0     |
| 3              | 5          | 1           | 1     |
| 1              | 15         | 0           | 0     |
| 5              | 8          | 1           | 1     |

**Модель:** P(спам) = σ(w₀ + w₁×восклиц + w₂×длина + w₃×бесплатно)

где σ(z) = 1/(1 + e⁻ᶻ)

**После обучения модели:**
w₀ = -2, w₁ = 0.5, w₂ = -0.1, w₃ = 3

**Пример предсказания:**
Письмо: 2 восклицательных знака, длина темы 12, есть слово "бесплатно"

z = -2 + 0.5×2 + (-0.1)×12 + 3×1 = -2 + 1 - 1.2 + 3 = 0.8
P(спам) = σ(0.8) = 1/(1 + e⁻⁰·⁸) ≈ 0.69

**Решение:** 69% вероятности спама → классифицируем как спам.

### 8.3 Пример 3: Многомерная регрессия

**Задача:** Предсказать стоимость аренды квартиры.

**Признаки:**
- Площадь (м²)
- Количество комнат  
- Расстояние до центра (км)
- Этаж

**Данные (упрощенные):**
| Площадь | Комнаты | Расстояние | Этаж | Цена (тыс./мес) |
|---------|---------|------------|------|------------------|
| 50      | 2       | 5          | 3    | 45               |
| 70      | 3       | 10         | 1    | 40               |
| 40      | 1       | 3          | 5    | 35               |
| 90      | 4       | 15         | 2    | 50               |

**Модель:**
Цена = w₀ + w₁×Площадь + w₂×Комнаты + w₃×Расстояние + w₄×Этаж

**После применения МНК:**
Цена = 20 + 0.3×Площадь + 5×Комнаты - 1×Расстояние + 0.5×Этаж

**Интерпретация весов:**
- w₁ = 0.3: каждый м² добавляет 300 руб. к аренде
- w₂ = 5: каждая дополнительная комната +5000 руб.
- w₃ = -1: каждый км от центра -1000 руб.
- w₄ = 0.5: каждый этаж выше +500 руб.

**Предсказание для новой квартиры (60 м², 2 комнаты, 8 км, 4 этаж):**
Цена = 20 + 0.3×60 + 5×2 - 1×8 + 0.5×4 = 20 + 18 + 10 - 8 + 2 = 42 тыс. руб.

### 8.4 Пример 4: Градиентный спуск в действии

**Простая задача:** минимизировать f(w) = w² - 4w + 3

**Аналитическое решение:**
f'(w) = 2w - 4 = 0 → w* = 2

**Градиентный спуск:**
```
Начальная точка: w₀ = 0
Скорость обучения: η = 0.3

Итерация 1:
f'(0) = 2×0 - 4 = -4
w₁ = 0 - 0.3×(-4) = 1.2

Итерация 2:  
f'(1.2) = 2×1.2 - 4 = -1.6
w₂ = 1.2 - 0.3×(-1.6) = 1.68

Итерация 3:
f'(1.68) = 2×1.68 - 4 = -0.64
w₃ = 1.68 - 0.3×(-0.64) = 1.872

...

Сходится к w* = 2
```

**Визуализация:**
```
f(w) |
     |     ╱
  3  |    ╱
     |   ╱  •w₀
  2  |  ╱    
     | ╱   •w₁
  1  |╱     •w₂
     |      •w₃
  0  |______|_______ w
     0   1  2*  3
```

## 9. Заключение

Линейные модели являются фундаментом машинного обучения, предоставляя простой и интерпретируемый подход к решению задач регрессии и классификации. Понимание основных концепций — выборки, признаков, весов, функций потерь и методов оптимизации — критично для дальнейшего изучения более сложных алгоритмов.

**Ключевые преимущества линейных моделей:**
- Простота и интерпретируемость
- Быстрое обучение
- Хорошие базовые результаты
- Устойчивость к переобучению

**Ограничения:**
- Предположение о линейности
- Чувствительность к выбросам (для MSE)
- Необходимость предобработки данных

**Дальнейшее развитие:**
- Регуляризация (Ridge, Lasso)
- Нелинейные преобразования признаков
- Ядерные методы
- Нейронные